{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Diabetes Prediction: Advanced Modeling\n",
        "\n",
        "In this notebook, we'll implement a stacked ensemble model using XGBoost and LightGBM as base models. We'll also explore feature importance using SHAP values and experiment with dimensionality reduction techniques like PCA and autoencoders.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.decomposition import PCA\n",
        "import shap\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
        "\n",
        "# Set plot style and size\n",
        "plt.style.use('ggplot')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Load Preprocessed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the preprocessed data\n",
        "print(\"Loading preprocessed data...\")\n",
        "try:\n",
        "    with open('data/preprocessed_data.pkl', 'rb') as f:\n",
        "        preprocessed_data = pickle.load(f)\n",
        "        \n",
        "    X_train = preprocessed_data['X_train']\n",
        "    X_test = preprocessed_data['X_test']\n",
        "    y_train = preprocessed_data['y_train']\n",
        "    y_test = preprocessed_data['y_test']\n",
        "    feature_names = preprocessed_data['feature_names']\n",
        "    \n",
        "    print(f\"Data loaded successfully. Training shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
        "    print(f\"Selected features: {feature_names}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading preprocessed data: {e}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Dimensionality Reduction Techniques\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 2.1 Principal Component Analysis (PCA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply PCA for feature compression\n",
        "print(\"Applying PCA for dimensionality reduction...\")\n",
        "\n",
        "# We'll try different numbers of components\n",
        "for n_components in [5, 7, 9]:\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "    \n",
        "    # Print explained variance ratio\n",
        "    explained_variance = np.sum(pca.explained_variance_ratio_)\n",
        "    print(f\"PCA with {n_components} components explains {explained_variance:.2%} of variance\")\n",
        "    \n",
        "    # Visualize the cumulative explained variance\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "    plt.axhline(y=0.95, color='r', linestyle='--', label='95% Explained Variance')\n",
        "    plt.axvline(x=n_components, color='g', linestyle='--', label=f'{n_components} Components')\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Cumulative Explained Variance')\n",
        "    plt.title(f'Explained Variance vs. Number of Components (n={n_components})')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Select the best PCA model (let's go with 7 components for a balance)\n",
        "pca = PCA(n_components=7)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "print(\"\\nComponent loadings (feature importance in each principal component):\")\n",
        "loadings_df = pd.DataFrame(pca.components_.T, index=X_train.columns)\n",
        "loadings_df.head()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 2.2 Autoencoder for Feature Compression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define and train an autoencoder for feature compression\n",
        "print(\"Training autoencoder for nonlinear dimensionality reduction...\")\n",
        "\n",
        "# Define the input dimension and encoding dimension\n",
        "input_dim = X_train.shape[1]\n",
        "encoding_dim = 7  # Same as PCA for comparison\n",
        "\n",
        "# Build the autoencoder model\n",
        "# Input layer\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "\n",
        "# Encoder\n",
        "encoder = Dense(12, activation='relu')(input_layer)\n",
        "encoder = BatchNormalization()(encoder)\n",
        "encoder = Dense(encoding_dim, activation='relu', name='bottleneck')(encoder)\n",
        "\n",
        "# Decoder\n",
        "decoder = Dense(12, activation='relu')(encoder)\n",
        "decoder = BatchNormalization()(decoder)\n",
        "decoder = Dense(input_dim, activation='linear')(decoder)\n",
        "\n",
        "# Create the autoencoder model\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Print model summary\n",
        "autoencoder.summary()\n",
        "\n",
        "# Train the autoencoder\n",
        "history = autoencoder.fit(\n",
        "    X_train, X_train,\n",
        "    epochs=50,\n",
        "    batch_size=256,\n",
        "    shuffle=True,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Autoencoder Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Create an encoder model to extract the bottleneck features\n",
        "encoder_model = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('bottleneck').output)\n",
        "\n",
        "# Get encoded features for train and test data\n",
        "X_train_encoded = encoder_model.predict(X_train)\n",
        "X_test_encoded = encoder_model.predict(X_test)\n",
        "\n",
        "print(f\"Original feature shape: {X_train.shape}\")\n",
        "print(f\"Encoded feature shape: {X_train_encoded.shape}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Model Training\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 3.1 Base Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to evaluate models\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name=\"Model\"):\n",
        "    \"\"\"Evaluate model performance with multiple metrics\"\"\"\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    \n",
        "    # For ROC AUC, we need probability predictions\n",
        "    try:\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "        auc = roc_auc_score(y_test, y_prob)\n",
        "    except AttributeError:\n",
        "        # If predict_proba is not available (e.g., for some models)\n",
        "        auc = None\n",
        "    \n",
        "    # Print the results\n",
        "    print(f\"\\n{model_name} Performance:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    if auc is not None:\n",
        "        print(f\"ROC AUC: {auc:.4f}\")\n",
        "        \n",
        "    # Create confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "    plt.title(f'{model_name} Confusion Matrix')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "    \n",
        "    # Return model and metrics\n",
        "    return {\n",
        "        'model': model,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'auc': auc\n",
        "    }\n",
        "\n",
        "# Initialize the base models with default parameters\n",
        "print(\"Initializing base models...\")\n",
        "\n",
        "# XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# LightGBM model\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    objective='binary',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train and evaluate XGBoost\n",
        "print(\"\\nTraining and evaluating XGBoost model...\")\n",
        "xgb_results = evaluate_model(xgb_model, X_train, X_test, y_train, y_test, \"XGBoost\")\n",
        "\n",
        "# Train and evaluate LightGBM\n",
        "print(\"\\nTraining and evaluating LightGBM model...\")\n",
        "lgb_results = evaluate_model(lgb_model, X_train, X_test, y_train, y_test, \"LightGBM\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 3.2 Models with PCA Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate models with PCA features\n",
        "print(\"\\nTraining models with PCA features...\")\n",
        "\n",
        "# XGBoost with PCA\n",
        "print(\"\\nTraining and evaluating XGBoost with PCA features...\")\n",
        "xgb_pca_results = evaluate_model(xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'), \n",
        "                               X_train_pca, X_test_pca, y_train, y_test, \"XGBoost with PCA\")\n",
        "\n",
        "# LightGBM with PCA\n",
        "print(\"\\nTraining and evaluating LightGBM with PCA features...\")\n",
        "lgb_pca_results = evaluate_model(lgb.LGBMClassifier(random_state=42), \n",
        "                              X_train_pca, X_test_pca, y_train, y_test, \"LightGBM with PCA\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 3.3 Models with Autoencoder Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate models with autoencoder features\n",
        "print(\"\\nTraining models with autoencoder features...\")\n",
        "\n",
        "# XGBoost with autoencoder features\n",
        "print(\"\\nTraining and evaluating XGBoost with autoencoder features...\")\n",
        "xgb_ae_results = evaluate_model(xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'), \n",
        "                             X_train_encoded, X_test_encoded, y_train, y_test, \"XGBoost with Autoencoder\")\n",
        "\n",
        "# LightGBM with autoencoder features\n",
        "print(\"\\nTraining and evaluating LightGBM with autoencoder features...\")\n",
        "lgb_ae_results = evaluate_model(lgb.LGBMClassifier(random_state=42), \n",
        "                            X_train_encoded, X_test_encoded, y_train, y_test, \"LightGBM with Autoencoder\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 3.4 Stacked Ensemble Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a stacked ensemble model\n",
        "print(\"\\nBuilding stacked ensemble model...\")\n",
        "\n",
        "# Define base models\n",
        "estimators = [\n",
        "    ('xgb', xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')),\n",
        "    ('lgb', lgb.LGBMClassifier(random_state=42))\n",
        "]\n",
        "\n",
        "# Define stacking classifier with a meta-learner\n",
        "stacked_model = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    cv=5,  # 5-fold cross-validation for training the meta-learner\n",
        "    stack_method='auto',  # Auto-detect whether to use predict_proba or decision_function\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train and evaluate stacked model\n",
        "print(\"\\nTraining and evaluating stacked ensemble model...\")\n",
        "stacked_results = evaluate_model(stacked_model, X_train, X_test, y_train, y_test, \"Stacked Ensemble\")\n",
        "\n",
        "# Compare all model results\n",
        "models = ['XGBoost', 'LightGBM', 'XGBoost+PCA', 'LightGBM+PCA', \n",
        "         'XGBoost+Autoencoder', 'LightGBM+Autoencoder', 'Stacked Ensemble']\n",
        "results = [xgb_results, lgb_results, xgb_pca_results, lgb_pca_results,\n",
        "          xgb_ae_results, lgb_ae_results, stacked_results]\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': models,\n",
        "    'Accuracy': [r['accuracy'] for r in results],\n",
        "    'Precision': [r['precision'] for r in results],\n",
        "    'Recall': [r['recall'] for r in results],\n",
        "    'F1 Score': [r['f1'] for r in results],\n",
        "    'AUC': [r['auc'] for r in results]\n",
        "})\n",
        "\n",
        "# Display model comparison\n",
        "print(\"\\n=== Model Performance Comparison ===\")\n",
        "comparison_df.set_index('Model').sort_values('F1 Score', ascending=False)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Feature Importance Analysis with SHAP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use SHAP to explain XGBoost predictions\n",
        "print(\"\\nCalculating SHAP values for XGBoost model...\")\n",
        "\n",
        "# Create explainer\n",
        "explainer = shap.TreeExplainer(xgb_results['model'])\n",
        "\n",
        "# Calculate SHAP values (sample 1000 data points for visualization if dataset is large)\n",
        "if len(X_test) > 1000:\n",
        "    shap_values = explainer.shap_values(X_test.sample(1000, random_state=42))\n",
        "    X_test_sample = X_test.sample(1000, random_state=42)\n",
        "else:\n",
        "    shap_values = explainer.shap_values(X_test)\n",
        "    X_test_sample = X_test\n",
        "\n",
        "# Summary plot\n",
        "plt.figure(figsize=(12, 10))\n",
        "shap.summary_plot(shap_values, X_test_sample, feature_names=X_test.columns.tolist())\n",
        "plt.title('SHAP Summary Plot for XGBoost Model')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Bar plot of mean absolute SHAP values\n",
        "plt.figure(figsize=(12, 10))\n",
        "shap.summary_plot(shap_values, X_test_sample, feature_names=X_test.columns.tolist(), plot_type=\"bar\")\n",
        "plt.title('SHAP Feature Importance Plot')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed SHAP analysis for top features\n",
        "print(\"\\nDetailed SHAP analysis for top features...\")\n",
        "\n",
        "# Calculate mean absolute SHAP values to identify top features\n",
        "mean_abs_shap = np.abs(shap_values).mean(0)\n",
        "feature_importance = pd.DataFrame(list(zip(X_test.columns, mean_abs_shap)),\n",
        "                                columns=['Feature', 'SHAP_Importance'])\n",
        "feature_importance = feature_importance.sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "# Display top 5 features\n",
        "print(\"Top 5 features by SHAP importance:\")\n",
        "print(feature_importance.head(5))\n",
        "\n",
        "# Create dependence plots for top 5 features\n",
        "top_features = feature_importance.head(5)['Feature'].values\n",
        "for feature in top_features:\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    feature_idx = list(X_test.columns).index(feature)\n",
        "    shap.dependence_plot(feature_idx, shap_values, X_test_sample, feature_names=X_test.columns)\n",
        "    plt.title(f'SHAP Dependence Plot for {feature}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create SHAP force plots for a few examples\n",
        "print(\"\\nSHAP force plots for individual examples:\")\n",
        "for i in range(3):  # Show 3 examples\n",
        "    plt.figure(figsize=(14, 4))\n",
        "    shap.force_plot(explainer.expected_value, shap_values[i,:], X_test_sample.iloc[i,:], \n",
        "                  feature_names=X_test.columns, matplotlib=True)\n",
        "    plt.title(f'SHAP Force Plot for Example {i+1}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use SHAP to explain LightGBM predictions\n",
        "print(\"\\nCalculating SHAP values for LightGBM model...\")\n",
        "\n",
        "# Create explainer\n",
        "explainer_lgb = shap.TreeExplainer(lgb_results['model'])\n",
        "\n",
        "# Calculate SHAP values\n",
        "if len(X_test) > 1000:\n",
        "    shap_values_lgb = explainer_lgb.shap_values(X_test.sample(1000, random_state=42))\n",
        "    X_test_sample = X_test.sample(1000, random_state=42)\n",
        "else:\n",
        "    shap_values_lgb = explainer_lgb.shap_values(X_test)\n",
        "    X_test_sample = X_test\n",
        "\n",
        "# Summary plot\n",
        "plt.figure(figsize=(12, 10))\n",
        "shap.summary_plot(shap_values_lgb, X_test_sample, feature_names=X_test.columns.tolist())\n",
        "plt.title('SHAP Summary Plot for LightGBM Model')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare feature importance between XGBoost and LightGBM\n",
        "# Calculate mean absolute SHAP values for LightGBM\n",
        "mean_abs_shap_lgb = np.abs(shap_values_lgb).mean(0)\n",
        "feature_importance_lgb = pd.DataFrame(list(zip(X_test.columns, mean_abs_shap_lgb)),\n",
        "                                   columns=['Feature', 'SHAP_Importance_LGB'])\n",
        "\n",
        "# Merge with XGBoost importance\n",
        "feature_importance_merged = feature_importance.merge(feature_importance_lgb, on='Feature')\n",
        "feature_importance_merged = feature_importance_merged.sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "# Display comparison\n",
        "print(\"\\nFeature importance comparison between XGBoost and LightGBM:\")\n",
        "print(feature_importance_merged.head(10))\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(12, 10))\n",
        "feature_importance_merged_top10 = feature_importance_merged.head(10)\n",
        "\n",
        "# Create a plot for comparison\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "x = np.arange(len(feature_importance_merged_top10))\n",
        "width = 0.35\n",
        "\n",
        "ax.bar(x - width/2, feature_importance_merged_top10['SHAP_Importance'], width, label='XGBoost')\n",
        "ax.bar(x + width/2, feature_importance_merged_top10['SHAP_Importance_LGB'], width, label='LightGBM')\n",
        "\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(feature_importance_merged_top10['Feature'], rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.set_title('Feature Importance Comparison: XGBoost vs LightGBM')\n",
        "ax.set_ylabel('Mean |SHAP Value|')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Conclusion and Model Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose the best performing model based on F1-Score\n",
        "best_model_idx = comparison_df['F1 Score'].idxmax()\n",
        "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
        "best_model_f1 = comparison_df.loc[best_model_idx, 'F1 Score']\n",
        "\n",
        "print(f\"The best performing model is {best_model_name} with an F1 Score of {best_model_f1:.4f}\")\n",
        "\n",
        "# Get all model metrics\n",
        "print(\"\\nPerformance metrics of all models:\")\n",
        "comparison_df = comparison_df.sort_values('F1 Score', ascending=False).reset_index(drop=True)\n",
        "comparison_df\n",
        "\n",
        "# Visualize comparison\n",
        "plt.figure(figsize=(14, 8))\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC']\n",
        "\n",
        "# Melt the dataframe for easier plotting\n",
        "melted_df = pd.melt(comparison_df, id_vars=['Model'], value_vars=metrics,\n",
        "                    var_name='Metric', value_name='Score')\n",
        "\n",
        "# Plot grouped bar chart\n",
        "sns.barplot(data=melted_df, x='Model', y='Score', hue='Metric')\n",
        "plt.title('Performance Comparison Across Models', fontsize=16)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylim(0.5, 1.0)  # Set y-axis to focus on the important range\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary of findings\n",
        "print(\"\\n--- Key Findings ---\")\n",
        "print(f\"1. Best model: {best_model_name} (F1 Score: {best_model_f1:.4f})\")\n",
        "\n",
        "# Top features from SHAP analysis\n",
        "print(\"\\n2. Top 5 most important features (based on SHAP values):\")\n",
        "for i, row in feature_importance.head(5).iterrows():\n",
        "    print(f\"   - {row['Feature']}: {row['SHAP_Importance']:.4f}\")\n",
        "\n",
        "# Compare dimensionality reduction approaches\n",
        "print(\"\\n3. Dimensionality Reduction Impact:\")\n",
        "original_f1 = max(xgb_results['f1'], lgb_results['f1'])\n",
        "pca_f1 = max(xgb_pca_results['f1'], lgb_pca_results['f1'])\n",
        "autoencoder_f1 = max(xgb_ae_results['f1'], lgb_ae_results['f1'])\n",
        "\n",
        "print(f\"   - Original features: F1 Score = {original_f1:.4f}\")\n",
        "print(f\"   - PCA features: F1 Score = {pca_f1:.4f} (Diff: {pca_f1 - original_f1:.4f})\")\n",
        "print(f\"   - Autoencoder features: F1 Score = {autoencoder_f1:.4f} (Diff: {autoencoder_f1 - original_f1:.4f})\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Save Best Model\n",
        "\n",
        "Save the best performing model for future deployment and use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model based on the comparison\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Map model names to their result dictionaries\n",
        "model_results_map = {\n",
        "    'XGBoost': xgb_results,\n",
        "    'LightGBM': lgb_results,\n",
        "    'XGBoost with PCA': xgb_pca_results,\n",
        "    'LightGBM with PCA': lgb_pca_results,\n",
        "    'XGBoost with Autoencoder': xgb_ae_results,\n",
        "    'LightGBM with Autoencoder': lgb_ae_results,\n",
        "    'Stacked Ensemble': stacked_results\n",
        "}\n",
        "\n",
        "# Get the best model\n",
        "best_model_result = model_results_map.get(best_model_name)\n",
        "\n",
        "if best_model_result:\n",
        "    # Save the model\n",
        "    best_model = best_model_result['model']\n",
        "    model_path = os.path.join('models', f\"{best_model_name.replace(' ', '_').lower()}.pkl\")\n",
        "    joblib.dump(best_model, model_path)\n",
        "    \n",
        "    # Save additional info if needed (for PCA or Autoencoder)\n",
        "    if 'PCA' in best_model_name:\n",
        "        joblib.dump(pca, os.path.join('models', 'pca_transformer.pkl'))\n",
        "    elif 'Autoencoder' in best_model_name:\n",
        "        encoder_model.save(os.path.join('models', 'autoencoder_encoder.h5'))\n",
        "        \n",
        "    # Also save the stacked ensemble model for deployment\n",
        "    joblib.dump(stacked_results['model'], os.path.join('models', 'stacked_ensemble.pkl'))\n",
        "    \n",
        "    print(f\"Best model ({best_model_name}) saved to {model_path}\")\n",
        "    print(f\"Stacked ensemble model also saved to models/stacked_ensemble.pkl\")\n",
        "else:\n",
        "    print(\"Could not find the best model to save.\")\n",
        "\n",
        "# Save feature importance information\n",
        "feature_importance.to_csv(os.path.join('models', 'feature_importance.csv'), index=False)\n",
        "print(\"Feature importance information saved to models/feature_importance.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
